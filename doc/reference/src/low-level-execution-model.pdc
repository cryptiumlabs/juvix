## Overview

Juvix translates the semantics of a term to equivalent semantics for an interaction system, consisting of node types, rewrite
rules, write-forward and read-back algorithms (for translating terms to and from nets, respectively),
where elementary-affine-typed terms are in the general case reduced using the oracle-free variant
of Lamping's optimal reduction algorithm.

Compared with previous interaction net interpreters for the lambda calculus utilizing a static set
of node types and fixed rewrite rules, Juvix adds an additional degree of freedom:
the node types and rewrite rules of the interaction system can be generated at compile time and even
dynamically altered at runtime according to patterns of rewrite combinations and desired time-space complexity tradeoffs.
Additional type data from the core language, such as exact variable usage counts provided by
the instantiation of quantitative type theory with the natural ring, are available to the interaction
system construction algorithm.

also
- refl (equality) proofs in core language can be used by compiler, e.g. with total supply of a token = constant, for queries on the total supply the constant can be returned; more generally if two expressions are equal the compiler can choose which one to evaluate
- will be more effective if graph representation is persistent, instead of written / read-back each contract call. can be used for both code & data

- Define encoding $\phi (t)$ of term $t$ mapping to net $n$
- Define read-back function $\phi ^{-1}(n)$ mapping net $n$ to a term $t$, where $\phi^{-1}(\phi(t)) = t$ holds
- Define interaction system reduction function $\psi(n)$ mapping nets to nets, where $\phi^{-1}(\psi(\phi(t))) = reduce\ t$ where $reduce$ is as defined in the semantics of Juvix Core

## Interaction system encoding

EAL term language $t ::= x\ |\ λx.t\ |\ (t u)\ |\ !t$.

EAL type $A ::= α\ |\ A\ ⊸\ A\ |\ !A$.

EAL-typed terms can be translated into interaction nets, in accordance with the sequent calculus typing rules, as the function $\phi$ as follows.

The EAL term is first erased to a simply-typed term, with EAL types and levels of subterms retained in a lookup table for reference during the translation.

**Abstraction** is applied to terms of the form $λx.t$ and type $A ⊸ B$.

\begin{tikzpicture}
\inetbigcell[right = 30pt]{g}{$\phi (t)$}[4]
\inetbigcell[below = 30pt]{l}{$λ$}[3]

\axWire{g/1}{l/1}{$(arg)$}{}
\cutWire{g.out}{l/2}{}{}
\outwire[]{l.out}{$A ⊸ B$}
\end{tikzpicture}

Wiring of the argument $x$ varies depending on variable usage linearity:

**Weakening**: If $x$ does not appear in the body $t$, the $λ$ argument port is connected to an eraser.

\begin{tikzpicture}
\inetbigcell[right = 30pt]{g}{$\phi (t)$}[4]
\inode[]{e}{$⊗$}
\inetbigcell[below = 30pt]{l}{$λ$}[3]

\cutWire{e}{l/1}{$$}{}
\cutWire{g.out}{l/2}{$$}{}
\outwire[]{l.out}{$A ⊸ B$}
\end{tikzpicture}

**Linear**: If *x* appears exactly once in the body $t$, the $λ$ argument port is connected directly to the occurrence. 

\begin{tikzpicture}
\inetbigcell[right = 30pt]{g}{$\phi (t)$}[4]
\inetbigcell[below = 30pt]{l}{$λ$}[3]

\axWire{g/1}{l/1}{}{}
\cutWire{g.out}{l/2}{}{}
\outwire[]{l.out}{$A ⊸ B$}
\end{tikzpicture}

**Contraction**: If *x* appears more than once in the body $t$, the $λ$ argument port is connected through a tree of fan-in nodes, each with a (globally) unique label, to all occurrences of $x$ in $t$ (only one fan-in node is shown in the diagram).

\begin{tikzpicture}
\inetbigcell[]{f}{$i$}[3]
\inetbigcell[right = 30pt]{g}{$\phi (t)$}[4]
\inetbigcell[below = 30pt]{l}{$λ$}[3]

\axWire{g/1}{f/1}{}{}
\axWire{g/2}{f/2}{}{}
\axWire{f.out}{l/1}{}{}
\cutWire{g.out}{l/2}{}{}
\outwire[]{l.out}{$A ⊸ B$}
\end{tikzpicture}

**Application** is applied to terms of the form $(t_1 t_2)$ and type $C$.

\begin{tikzpicture}

\inetbigcell[right = -25 pt]{g}{$\phi(t_1)$}[3]
\inetbigcell[right = 25 pt]{h}{$\phi(t_2)$}[3]
\inetbigcell[below = 40 pt]{a}{$@$}[3]

\cutWire{g.out}{a/1}{}{}
\cutWire{h.out}{a/2}{}{}
\outwire[]{a.out}{$C$}

\end{tikzpicture}

That ends the encoding rules.

Example from chat & paper (of derivation).

## Oracle-free optimal reduction

## Bespoke term encoding

- Over set of underlying opcodes & von-Neumann-ish VM (e.g. x86, LLVM, EVM).
- Use linear types (e.g. rewrite memory locations in place)
- Must be semantically equivalent to interaction system encoding after read-back
- Must preserve any-order reduction (exists some maximium `n` rewrite steps after which graph read-back must have final form regardless of order)
- Can relax diamond property (strong confluence)? unclear if necessary

## Evaluator cost model

Currently tracked:

- Memory allocations
- Sequential rewrite steps
- Parallel rewrite steps
- Maximum graph size
- Final graph size

In the future we may want to track more granular memory operations (reads/writes in addition to allocations) and computations associated with rewrite rules (all negligible-cost with interaction combinators, but not with e.g. integer arithmetic).

Machine backends are expected to provide a discrete cost model which can be utilized by the optimizer.

## Future optimization strategies

Juvix does not yet implement these, but the compiler architecture has kept their possibility in mind.

### Spacial memory contiguity

Random access O(1) model is imperfect; sequential reads are faster. Ensure correspondence between graphical locality and spacial locality in memory, read nodes in blocks.

### Speculative execution

- "Strict" optimal reduction strategies
- Evaluate based on predicting future input (feasible?)

### Stochastic superoptimization

- Utilize sparse sampling (probably Markov-chain Monte Carlo) to search the configuration space of semantically equivalent programs & select the fastest.
- Probably useful at the level of choosing machine implementations of particular rewrite rules.
- See Stochastic Superoptimization [@stochastic-superoptimization]
- Will need a lot of clever tricks to avoid getting stuck in local minima (that paper details several).
- See also STOKE [@stoke]

### "Superoptimal" reduction strategies

- Specifically those with the possibility of asymptotically-better performance than Levy's optimal reduction.
- As far as I can tell, the only candidates here are forms of memoization which attempt to detect syntactically identical structures during the reduction process which can then be linked and evaluated only once.
- [Hash consing](https://en.wikipedia.org/wiki/Hash_consing) may have the most prior research.
- Concerns about space-time tradeoffs (may already be concerns).
